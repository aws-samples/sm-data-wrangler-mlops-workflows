{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Step Functions Workflow from a Data Wrangler Flow File\n",
    "\n",
    "This notebook creates a Step Functions workflow that runs a data preparation step based on the flow file configuration, training step to train a XGBoost model artifact and creates a SageMaker model from the artifact. \n",
    "\n",
    "Before proceeding with this notebook, please ensure that you have executed the 00_setup_data_wrangler.ipynb. This notebook uploads the input files, creates the flow file locally from the insurance claims template and uploads flow file to the default S3 bucket associated with the Studio domain.\n",
    "\n",
    "This notebook is created using the export feature in Data Wrangler and modified and parameterized for Step functions workflow with SageMaker data science SDK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we read the input workflow parameters with a predefined schema and use this in our notebook. A sagemaker workflow requires job names to be unique. So we pass on the below names as parameters by generating random ids when we execute the workflow towards the end of the notebook.\n",
    "\n",
    "1. Processing JobName\n",
    "2. Training JobName\n",
    "3. Model JobName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use your own bucket name as this bucket will be used for creating the training dataset and the model artifact. The flow file is also generated locally and contains the S3 location details for the two input files namely claims.csv and customer.csv. The flow file is available as a JSON document and holds all the input, output and transformation details in a node structure. SageMaker jobs need the inputs to be available in S3. So, we derive the input file S3 locations from the flow file by reading the JSON document and building an array with a Processing Input object for SageMaker processing.\n",
    "\n",
    "You can configure your output location as you wish but SageMaker processing job requires the output name to match with the one created in the flow file. Any mismatch can fail the job. So, we extract the output name from the flow file as below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ins_claim_flow_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import string\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# bucket = <YOUR_BUCKET>\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'aws-data-wrangler-workflows'\n",
    "\n",
    "FLOW_TEMPLATE_URI = ins_claim_flow_uri\n",
    "\n",
    "flow_file_name = FLOW_TEMPLATE_URI.split(\"/\")[-1]\n",
    "flow_export_name = flow_file_name.replace(\".flow\", \"\")\n",
    "flow_export_id = flow_export_name.replace(\"flow-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader.download(FLOW_TEMPLATE_URI, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(flow_file_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    output_node = data['nodes'][-1]['node_id']\n",
    "    print(output_node)\n",
    "    output_path = data['nodes'][-1]['outputs'][0]['name']\n",
    "    input_source_names = [node['parameters']['dataset_definition']['name'] for node in data['nodes'] if node['type']==\"SOURCE\"]\n",
    "    input_source_uris = [node['parameters']['dataset_definition']['s3ExecutionContext']['s3Uri'] for node in data['nodes'] if node['type']==\"SOURCE\"]\n",
    "\n",
    "output_name = f\"{output_node}.{output_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "data_sources = []\n",
    "\n",
    "for i in range(0,len(input_source_uris)):\n",
    "    data_sources.append(ProcessingInput(\n",
    "        source=input_source_uris[i],\n",
    "        destination=f\"/opt/ml/processing/{input_source_names[i]}\",\n",
    "        input_name=input_source_names[i],\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\"\n",
    "    ))\n",
    "    \n",
    "print(data_sources)\n",
    "\n",
    "s3_output_prefix = f\"preprocessing/output\"\n",
    "s3_training_dataset = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_training_dataset,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow file is created and uploaded into S3 as part of the setup notebook. If you encounter any error in the below cell,please go back to the Setup notebook to make sure flow file is generated and uploaded correctly. Now, we retrieve the flow file s3 uri that was stored as a global variable in the setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ins_claim_flow_uri\n",
    "ins_claim_flow_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing job also needs to access the flow file to run the transformations. So, we provide the flow file location as another input source for the processing job as below by passing the s3 uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: insurance_claims.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=ins_claim_flow_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a processing job\n",
    "\n",
    "Please follow the steps below for creating an execution role with the right permissions for the workflow.\n",
    "\n",
    "1. Go to the IAM Console - Roles. Choose Create role. \n",
    "\n",
    "2. For role type, choose AWS Service, find and choose SageMaker, and choose Next: Permissions \n",
    "\n",
    "3. On the Attach permissions policy page, choose (if not already selected) \n",
    "\n",
    "    a. AWS managed policy AmazonSageMakerFullAccess\n",
    "    \n",
    "    b. AWS managed policy AmazonS3FullAccess for access to Amazon S3 resources\n",
    "    \n",
    "    c. AWS managed policy CloudWatchEventsFullAccess\n",
    "\n",
    "4. Then choose Next: Tags and then Next: Review.\n",
    "\n",
    "5. For Role name, enter StepFunctionsSageMakerExecutionRole and Choose Create Role\n",
    "\n",
    "6. Additionally, we need to add step functions as a trusted entity to the role. Go to trust relationships in the specific IAM role and edit it to add  states.amazonaws.com (http://states.amazonaws.com/) as a trusted entity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the role, we go on to configure the inputs required by the SageMaker Python SDK to launch a processing job. You can change it as per your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = sagemaker.Session()\n",
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "aws_region = sess.boto_region_name\n",
    "\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='data-wrangler',\n",
    "    region=aws_region\n",
    ")\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# KMS key for per object encryption; default is None\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Processor\n",
    "\n",
    "To launch a Processing Job, we will use the SageMaker Python SDK to create a Processor function with the configuration set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Step Function WorkFlow \n",
    "## Define Steps\n",
    "A step function workflow consists of multiple steps that run as separate state machines. We will first create a `ProcessingStep` using the Data Wrangler processor defined above. Processing job name is unique and passed on as a command line argument from the workflow. This is used to track and monitor the job in console and cloudwatch logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.inputs import ExecutionInput\n",
    "workflow_parameters = ExecutionInput(schema={\"ProcessingJobName\": str, \"TrainingJobName\": str,\"ModelName\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import ProcessingStep\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    \"WranglerStepFunctionsProcessingStep\",\n",
    "    processor=processor,\n",
    "    job_name = workflow_parameters[\"ProcessingJobName\"],\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add a TrainingStep to the workflow that trains a model on the preprocessed train data set. Here we use a builtin XG boost algorithm with fixed hyperparameters. You can configure the training based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.2-1\",\n",
    "        py_version=\"py3\",\n",
    "        instance_type=instance_type,\n",
    "    )\n",
    "xgb_train = Estimator(\n",
    "        image_uri=image_uri,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        role=iam_role,\n",
    "    )\n",
    "xgb_train.set_hyperparameters(\n",
    "        objective=\"reg:squarederror\",\n",
    "        num_round=3,\n",
    "    )\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from stepfunctions.steps import TrainingStep\n",
    "\n",
    "xgb_input_content_type = 'text/csv'\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    \"WranglerStepFunctionsTrainingStep\",\n",
    "    estimator=xgb_train,\n",
    "    data={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=s3_training_dataset,\n",
    "            content_type=xgb_input_content_type\n",
    "        )\n",
    "    },\n",
    "    job_name = workflow_parameters[\"TrainingJobName\"],\n",
    "    wait_for_completion=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training job will produce a model artifact. As a final step, we register this artifact as a SageMaker model with the model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import ModelStep\n",
    "\n",
    "model_step = ModelStep(\n",
    "    \"SaveModelStep\", model=training_step.get_expected_model(), model_name=workflow_parameters[\"ModelName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to notify failures, we need to configure a fail step with an error message as below and call it during the incidence of every failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Fail\n",
    "\n",
    "process_failure = Fail(\n",
    "    \"Step Functions Wrangler Workflow failed\", cause=\"Wrangler-StepFunctions-Workflow failed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be situations where you may expect intermittent failures due to unavailable resources and may want to retry a specific step. You can set up retry mechanism as below for such steps and configure the interval and the attempts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Retry\n",
    "\n",
    "data_wrangler_step.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=15,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=3.0\n",
    "))\n",
    "\n",
    "training_step.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=10,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=4.0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we can introduce a wait interval between steps to ensure a smooth transition and provide a specific step with all the resources and inputs it will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Wait\n",
    "\n",
    "wait_step = Wait(\n",
    "    state_id=\"Wait for 3 seconds\",\n",
    "    seconds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the steps needed for the workflow, we need to catch and notify failures at every step. We fail the entire workflow whenever a failure is caught by this FailStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps.states import Catch\n",
    "\n",
    "catch_failure = Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=process_failure\n",
    ")\n",
    "\n",
    "data_wrangler_step.add_catch(catch_failure)\n",
    "training_step.add_catch(catch_failure)\n",
    "model_step.add_catch(catch_failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a workflow by chaining all the above defined steps in order and execute it with the required parameters. Unique job names are generated randomly and passed on as parameters for the workflow. We introduce a wait time of 3 steps between preprocessing and training steps by chaining the wait step in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import Chain\n",
    "from stepfunctions.workflow import Workflow\n",
    "import uuid\n",
    "\n",
    "workflow_graph = Chain([data_wrangler_step, wait_step,training_step, model_step])\n",
    "\n",
    "branching_workflow = Workflow(\n",
    "    name=\"Wrangler-SF-Run-{}\".format(uuid.uuid1().hex),\n",
    "    definition=workflow_graph,\n",
    "    role=iam_role\n",
    ")\n",
    "\n",
    "branching_workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Preprocessing job requires a unique name\n",
    "processing_job_name = \"wrangler-sf-processing-{}\".format(\n",
    "    uuid.uuid1().hex)\n",
    "# Each Training Job requires a unique name\n",
    "training_job_name = \"wrangler-sf-training-{}\".format(\n",
    "    uuid.uuid1().hex)\n",
    "model_name = \"sf-claims-fraud-model-{}\".format(uuid.uuid1().hex)\n",
    "\n",
    "\n",
    "# Execute workflow\n",
    "execution = branching_workflow.execute(\n",
    "    inputs={\n",
    "         \"ProcessingJobName\": processing_job_name, # Each pre processing job (SageMaker processing job) requires a unique name,\n",
    "         \"TrainingJobName\": training_job_name,#  Each Sagemaker Training job requires a unique name,\n",
    "         \"ModelName\" : model_name # Each model requires a unique name\n",
    "    } \n",
    ")\n",
    "execution_output = execution.get_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the step function workflow status and details in the Step Functions console. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) StepFunctions cleanup\n",
    "1. Delete the input claims file, customer file and flow file from S3.\n",
    "2. Delete the training dataset created by processing job\n",
    "3. Delete the model artifact tar file from S3.\n",
    "4. Delete the SageMaker Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
