{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Integrate Data Wrangler with Apache Airflow ML Pipeline\n",
    "\n",
    "<div class=\"alert alert-warning\"> ⚠️ <strong> PRE-REQUISITE: </strong>\n",
    "\n",
    "Before proceeding with this notebook, please ensure that you have \n",
    "    \n",
    "1. Executed the <code>00_setup_data_wrangler.ipynb</code> Notebook</li>\n",
    "2. Created an <a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\" target=\"_blank\">Amazon Managed Workflow for Apache Airflow (MWAA)</a> environment. Please visit the Amazon MWAA <a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/get-started.html\" target=\"_blank\">Get started</a> documentation to see how you can create an MWAA environment. Alternatively, to quickly get started with MWAA, follow the <a href=\"https://catalog.us-east-1.prod.workshops.aws/v2/workshops/795e88bb-17e2-498f-82d1-2104f4824168/en-US/workshop-2-0-2/setup/mwaa\" target=\"_blank\">step-by-step instructions</a> in the MWAA workshop to setup an MWAA environment.\n",
    "\n",
    "</div>\n",
    "\n",
    "This notebook creates the required scripts for the Apache Airflow workflow and uploads them to the respective S3 bucket locations for MWAA. We will create-\n",
    "\n",
    "1. A `requirements.txt` file and upload it to the MWAA `/requirements` prefix\n",
    "2. We upload the `SMDataWranglerOperator.py` Python script which is the SageMaker Data Wrangler custom Airflow Operator to the `/dags` prefix.\n",
    "2. A `config.py` Python script that will setup configurations for our DAG Tasks and upload to the `/dags` prefix.\n",
    "3. And finally, we create an `ml_pipeline.py` Python script which sets up the end-to-end Apache Airflow workflow DAG and upload it to the `/dags` prefix.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required dependencies and initialize variables\n",
    "\n",
    "<div class=\"alert alert-warning\"> ⚠️ <strong> NOTE: </strong>\n",
    "    Note: replace <code>bucket</code> name with your MWAA Bucket name.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.59.5\n",
      "S3 bucket: airflow-data-wrangler\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "import boto3\n",
    "import string\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# MWAA Client\n",
    "mwaa_client = boto3.client('mwaa')\n",
    "\n",
    "# Replace the bucket name with your MWAA Bucket\n",
    "bucket = 'airflow-data-wrangler'\n",
    "\n",
    "print(f'SageMaker version: {sagemaker.__version__}')\n",
    "print(f'S3 bucket: {bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and uploading this `.airflowignore` file helps Airflow to prevent interpreting the helper Python scripts as a DAG file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/.airflowignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/.airflowignore\n",
    "SMDataWranglerOperator\n",
    "config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"scripts/.airflowignore\", bucket, f\"dags/.airflowignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create `requirements.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `requirements.txt` file and upload it to S3. We will need a few dependencies to be able to run our Data Wrangler python script using the Apache Airflow Python operator, mainly the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "awswrangler\n",
    "pandas\n",
    "sagemaker==v2.59.5\n",
    "dag-factory==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"scripts/requirements.txt\", bucket, f\"requirements/requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload the custom SageMaker Data Wrangler Operator\n",
    "\n",
    "In this step we will upload the [custom Airflow operator](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html) for SageMaker Data Wrangler. With this operator, you can pass in any SageMaker Data Wrangler `.flow` file to Airflow to perform data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"scripts/SMDataWranglerOperator.py\", bucket, f\"dags/SMDataWranglerOperator.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update MWAA IAM Execution Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every MWAA Environment has an [Execution Role](https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-create-role.html) attached to it. This role consists of permissions policy that grants Amazon Managed Workflows for Apache Airflow (MWAA) permission to invoke the resources of other AWS services on your behalf. In our case, we want our MWAA Tasks to be able to access SageMaker and S3. Edit the MWAA Execution role and add the permissions listed below-\n",
    "\n",
    "- `AmazonS3FullAccess`\n",
    "- `AmazonSageMakerFullAccess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker Role for MWAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a SageMaker service role to be used in the ML pipeline module. To create an IAM role for Amazon SageMaker\n",
    "\n",
    "- Go to the IAM Console - Roles\n",
    "- Choose Create role\n",
    "- For role type, choose AWS Service, find and choose SageMaker, and choose Next: Permissions\n",
    "- On the Attach permissions policy page, choose (if not already selected)\n",
    "  - AWS managed policy `AmazonSageMakerFullAccess`\n",
    "  - AWS managed policy `AmazonS3FullAccess` for access to Amazon S3 resources\n",
    "- Then choose Next: Tags and then Next: Review.\n",
    "- For Role name, enter AirflowSageMakerExecutionRole and Choose Create Role\n",
    "\n",
    "Alternatively, we can also use the default SageMaker Execution role since it already has these permissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::965425568475:role/service-role/AmazonSageMaker-ExecutionRole-20201030T135016'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iam_role = sagemaker.get_execution_role()\n",
    "iam_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup configuration script\n",
    "\n",
    "In this step we create a helper script to define the model training and model creation task configurations. This script will be used by the DAG tasks to obtain various configuration information for model training and model creation.\n",
    "\n",
    "<div class=\"alert alert-warning\"> ⚠️ <strong> NOTE: </strong>\n",
    "    Note: replace <code>bucket</code> with the SageMaker Default bucket name for your SageMaker studio domain.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/config.py\n",
    "#!/usr/bin/env python\n",
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "import json\n",
    "from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "def config(**opts):\n",
    "    \n",
    "    region_name=opts['region_name'] if 'region_name' in opts else sagemaker.Session().boto_region_name\n",
    "    \n",
    "    # Hook\n",
    "    hook = AwsBaseHook(aws_conn_id='airflow-sagemaker', resource_type=\"sagemaker\")\n",
    "    boto_session = hook.get_session(region_name=region_name)\n",
    "    sagemaker_session =  sagemaker.session.Session(boto_session=boto_session)\n",
    "    \n",
    "\n",
    "    training_job_name=opts['training_job_name']\n",
    "    bucket = opts['bucket'] if 'bucket' in opts else sagemaker_session.default_bucket() #\"sagemaker-us-east-2-965425568475\"\n",
    "    s3_prefix = opts['s3_prefix']\n",
    "    # Get the xgboost container uri\n",
    "    container = get_image_uri(region_name, 'xgboost', repo_version='1.0-1')\n",
    "    \n",
    "    ts = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"   \n",
    "    config = {}\n",
    "    \n",
    "    config[\"data_wrangler_config\"] = {        \n",
    "        \"sagemaker_role\":             opts['role_name'],\n",
    "        #\"s3_data_type\"              :    defaults to \"S3Prefix\" \n",
    "        #\"s3_input_mode\"             :    defaults to \"File\", \n",
    "        #\"s3_data_distribution_type\" :    defaults to \"FullyReplicated\", \n",
    "        #\"aws_conn_id\"               :    defaults to \"aws_default\",\n",
    "        #\"kms_key\"                   :    defaults to None, \n",
    "        #\"volume_size_in_gb\"         :    defaults to 30,\n",
    "        #\"enable_network_isolation\"  :    defaults to False, \n",
    "        #\"wait_for_processing\"       :    defaults to True, \n",
    "        #\"container_uri\"             :    defaults to \"415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.x\", \n",
    "        #\"container_uri_pinned\"      :    defaults to \"415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.12.0\",  \n",
    "        \"outputConfig\": {\n",
    "              #\"s3_output_upload_mode\":     #defaults to EndOfJob\n",
    "              #\"output_content_type\":       #defaults to CSV\n",
    "              #\"output_bucket\":             #defaults to SageMaker Default bucket\n",
    "              \"output_prefix\": s3_prefix     #prefix within bucket where output will be written, default is generated automatically\n",
    "        }\n",
    "    }\n",
    "\n",
    "    config[\"train_config\"]={\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": container,\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        \"HyperParameters\": {\n",
    "            \"max_depth\": \"5\",\n",
    "            \"num_round\": \"10\",\n",
    "            \"objective\": \"reg:squarederror\"\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"ContentType\": \"csv\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": f\"s3://{bucket}/{s3_prefix}/train\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": f\"s3://{bucket}/{s3_prefix}/xgboost\"\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.m5.2xlarge\",\n",
    "            \"VolumeSizeInGB\": 5\n",
    "        },\n",
    "        \"RoleArn\": opts['role_name'],\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 86400\n",
    "        },\n",
    "        \"TrainingJobName\": training_job_name\n",
    "    }\n",
    "    \n",
    "    config[\"model_config\"]={\n",
    "           \"ExecutionRoleArn\": opts['role_name'],\n",
    "           \"ModelName\": f\"XGBoost-Fraud-Detector-{ts}\",\n",
    "           \"PrimaryContainer\": { \n",
    "              \"Mode\": \"SingleModel\",\n",
    "              \"Image\": container,\n",
    "              \"ModelDataUrl\": f\"s3://{bucket}/{s3_prefix}/xgboost/{training_job_name}/output/model.tar.gz\"\n",
    "           },\n",
    "        }\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload `config.py` to the `/dags` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"scripts/config.py\", bucket, f\"dags/config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Apache Airflow DAG (Directed Acyclic Graph)\n",
    "\n",
    "In this step, we will create the Python script to setup the Apache Airflow DAG. The script will create three distinct tasks and finally chain them together using `>>` in the end to create the Airflow DAG.\n",
    "\n",
    "1. Use Python operator to define a task to run the Data Wrangler script for data pre-processing\n",
    "2. Use SageMaker operator to define a task to train an XGBoost model using the training data\n",
    "3. Use SageMaker operator to define a task to create a model using the model artifacts created by the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-965425568475/data-wrangler-pipeline/flow/flow-21-08-44-46-aea6f365.flow'"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r ins_claim_flow_uri\n",
    "ins_claim_flow_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/ml_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/ml_pipeline.py\n",
    "#!/usr/bin/env python\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Import config file.\n",
    "from config import config\n",
    "from datetime import timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.models import DAG\n",
    "\n",
    "# airflow operators\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# airflow sagemaker operators\n",
    "from airflow.providers.amazon.aws.operators.sagemaker_training import SageMakerTrainingOperator\n",
    "from airflow.providers.amazon.aws.operators.sagemaker_model import SageMakerModelOperator\n",
    "\n",
    "# airflow sagemaker configuration\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.airflow import training_config\n",
    "\n",
    "# airflow Data Wrangler operator\n",
    "from SMDataWranglerOperator import SageMakerDataWranglerOperator\n",
    "\n",
    "# airflow dummy operator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "\n",
    "\n",
    "  \n",
    "default_args = {  \n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': airflow.utils.dates.days_ago(1),\n",
    "    'retries': 0,\n",
    "    'retry_delay': timedelta(minutes=2),\n",
    "    'provide_context': True,\n",
    "    'email': ['airflow@iloveairflow.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False\n",
    "}\n",
    "ts                = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "DAG_NAME          = f\"ml-pipeline\"\n",
    "\n",
    "#-------\n",
    "### Start creating DAGs\n",
    "#-------\n",
    "\n",
    "dag = DAG(  \n",
    "            DAG_NAME,\n",
    "            default_args=default_args,\n",
    "            dagrun_timeout=timedelta(hours=2),\n",
    "            # Cron expression to auto run workflow on specified interval\n",
    "            # schedule_interval='0 3 * * *'\n",
    "            schedule_interval=None\n",
    "        )\n",
    "\n",
    "#-------\n",
    "# Task to create configurations\n",
    "#-------\n",
    "\n",
    "config_task = PythonOperator(\n",
    "        task_id = 'Start',\n",
    "        python_callable=config,\n",
    "        op_kwargs={\n",
    "            'training_job_name': f\"XGBoost-training-{ts}\",\n",
    "            's3_prefix': 'data-wrangler-pipeline',\n",
    "            'role_name': 'AmazonSageMaker-ExecutionRole-20201030T135016'},\n",
    "        provide_context=True,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "#-------\n",
    "# Task with SageMakerDataWranglerOperator operator for Data Wrangler Processing Job.\n",
    "#-------\n",
    "\n",
    "def datawrangler(**context):\n",
    "    config = context['ti'].xcom_pull(task_ids='Start',key='return_value')\n",
    "    preprocess_task = SageMakerDataWranglerOperator(\n",
    "                            task_id='DataWrangler_Processing_StepNew',\n",
    "                            dag=dag,\n",
    "                            flow_file_s3uri=\"$flow_uri\",\n",
    "                            processing_instance_count=2,\n",
    "                            instance_type='ml.m5.4xlarge',\n",
    "                            aws_conn_id=\"aws_default\",\n",
    "                            config= config[\"data_wrangler_config\"]\n",
    "                    )\n",
    "    preprocess_task.execute(context)\n",
    "\n",
    "datawrangler_task = PythonOperator(\n",
    "        task_id = 'SageMaker_DataWrangler_step',\n",
    "        python_callable=datawrangler,\n",
    "        provide_context=True,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "#-------\n",
    "# Task with SageMaker training operator to train the xgboost model\n",
    "#-------\n",
    "\n",
    "def trainmodel(**context):\n",
    "    config = context['ti'].xcom_pull(task_ids='Start',key='return_value')\n",
    "    trainmodel_task = SageMakerTrainingOperator(\n",
    "                task_id='Training_Step',\n",
    "                config= config['train_config'],\n",
    "                aws_conn_id='aws-sagemaker',\n",
    "                wait_for_completion=True,\n",
    "                check_interval=30\n",
    "            )\n",
    "    trainmodel_task.execute(context)\n",
    "\n",
    "train_model_task = PythonOperator(\n",
    "        task_id = 'SageMaker_training_step',\n",
    "        python_callable=trainmodel,\n",
    "        provide_context=True,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "#-------\n",
    "# Task with SageMaker Model operator to create the xgboost model from artifacts\n",
    "#-------\n",
    "\n",
    "def createmodel(**context):\n",
    "    config = context['ti'].xcom_pull(task_ids='Start',key='return_value')\n",
    "    createmodel_task= SageMakerModelOperator(\n",
    "            task_id='Create_Model',\n",
    "            config= config['model_config'],\n",
    "            aws_conn_id='aws-sagemaker',\n",
    "        )\n",
    "    createmodel_task.execute(context)\n",
    "    \n",
    "create_model_task = PythonOperator(\n",
    "        task_id = 'SageMaker_create_model_step',\n",
    "        python_callable=createmodel,\n",
    "        provide_context=True,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "#------\n",
    "# Last step\n",
    "#------\n",
    "end_task = DummyOperator(task_id='End', dag=dag)\n",
    "\n",
    "# Create task dependencies\n",
    "\n",
    "config_task >> datawrangler_task >> train_model_task >> create_model_task >> end_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `$flow_uri` in the `ml_pipeline.py` script with the store magic variable `ins_claim_flow_uri` which contains S3 path of the `.flow` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ml_pipeline.py\", 'r') as f:\n",
    "    variables   = {'flow_uri': ins_claim_flow_uri}\n",
    "    template    = string.Template(f.read())\n",
    "    ml_pipeline = template.substitute(variables)\n",
    "\n",
    "# Creates the .flow file\n",
    "with open('ml_pipeline.py', 'w') as f:\n",
    "    f.write(ml_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload `ml_pipeline.py` to the `/dags` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"scripts/ml_pipeline.py\", bucket, f\"dags/ml_pipeline.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## View Airflow DAG and run\n",
    "\n",
    "Once the above steps are complete, you can access the [Apache Airflow UI](https://docs.aws.amazon.com/mwaa/latest/userguide/access-airflow-ui.html) and view the DAG. To access the Apache Airflow UI, go to the Amazon MWAA Console, select the MWAA Environment and click the _Airflow UI_ link.\n",
    "\n",
    "<img src=\"images/mwaa_ui.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "You can run the DAG by clicking on the \"Play\" button, alternatively you can -\n",
    "\n",
    "1. Setup the DAG to run on a set schedule automatically using cron expressions\n",
    "2. Setup the DAG to run based on S3 sensors such that the pipeline/workflow would execute whenever a new file arrives in a bucket/prefix.\n",
    "\n",
    "<img src=\"images/mwaa_dag.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "1. Delete the MWAA Environment from the Amazon MWAA Console.\n",
    "2. Delete the MWAA S3 files.\n",
    "3. Delete the Model training data and model artifact files from S3.\n",
    "4. Delete the SageMaker Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "We created an ML Pipeline with Apache Airflow and used the Data Wrangler script to pre-process and generate new training data for our model training and subsequently created a new model in Amazon SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
