{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Creating SageMaker Processing Job from AWS Data Wrangler Flow Template\n",
    "\n",
    "<div class=\"alert alert-warning\"> \n",
    "\t⚠️ <strong> PRE-REQUISITE: </strong> Before proceeding with this notebook, please ensure that you have executed the <code>00_setup_data_wrangler.ipynb</code> Notebook</li>\n",
    "</div>\n",
    "\n",
    "We will demonstrate how to define a SageMaker Processing Job based on an existing SageMaker Data Wrangler Flow definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ins_claim_flow_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import string\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'aws-data-wrangler-workflows'\n",
    "\n",
    "FLOW_TEMPLATE_URI = ins_claim_flow_uri\n",
    "\n",
    "flow_file_name = FLOW_TEMPLATE_URI.split(\"/\")[-1]\n",
    "flow_export_name = flow_file_name.replace(\".flow\", \"\")\n",
    "flow_export_id = flow_export_name.replace(\"flow-\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Flow template from Amazon S3\n",
    "\n",
    "We download the flow template from S3, in order to parse its content and retrieve the following information:\n",
    "* Source datasets, including dataset names and S3 URI\n",
    "* Output node, including Node ID and output path\n",
    "This information is then used as part of the parameters of the SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader.download(FLOW_TEMPLATE_URI, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing input and output parameters from flow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(flow_file_name, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    output_node = data['nodes'][-1]['node_id']\n",
    "    output_path = data['nodes'][-1]['outputs'][0]['name']\n",
    "    input_source_names = [node['parameters']['dataset_definition']['name'] for node in data['nodes'] if node['type']==\"SOURCE\"]\n",
    "    input_source_uris = [node['parameters']['dataset_definition']['s3ExecutionContext']['s3Uri'] for node in data['nodes'] if node['type']==\"SOURCE\"]\n",
    "    \n",
    "\n",
    "output_name = f\"{output_node}.{output_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create SageMaker Processing Job from Data Wrangler Flow template\n",
    "\n",
    "### 3.1 SageMaker Processing Inputs\n",
    "\n",
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job.\n",
    "\n",
    "#### Source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "data_sources = []\n",
    "\n",
    "for i in range(0,len(input_source_uris)):\n",
    "    data_sources.append(ProcessingInput(\n",
    "        source=input_source_uris[i],\n",
    "        destination=f\"/opt/ml/processing/{input_source_names[i]}\",\n",
    "        input_name=input_source_names[i],\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=FLOW_TEMPLATE_URI,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")\n",
    "\n",
    "processing_job_inputs=[flow_input] + data_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SageMaker Processing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")\n",
    "\n",
    "processing_job_outputs=[processing_job_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create Processor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "aws_region = sess.boto_region_name\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URI.\n",
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='data-wrangler',\n",
    "    region=aws_region\n",
    ")\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# KMS key for per object encryption; default is None\n",
    "kms_key = None\n",
    "\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "processing_job_output_content_type = \"CSV\"\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "processing_job_output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": processing_job_output_content_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the Processing Job in a workflow compatible with SageMaker SDK, you will create a Processor function. The processor can then be integrated in the following workflows:\n",
    "\n",
    "* Amazon SageMaker Pipelines\n",
    "* AWS Step Functions (through AWS Step Functions Data Science SDK)\n",
    "* Apache Airflow (through a Python Operator, using Amazon SageMaker SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create SageMaker Estimator\n",
    "\n",
    "Another building block for orchestrating ML workflows is an Estimator, which is the base for a Training Job. The estimator can be integrated in the following workflows:\n",
    "\n",
    "* Amazon SageMaker Pipelines\n",
    "* AWS Step Functions (through AWS Step Functions Data Science SDK)\n",
    "* Apache Airflow (through Amazon SageMaker Operator for Apache Airflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Estimator Instance count and instance type.\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    role=iam_role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",\n",
    "    num_round=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating a workflow with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Pipeline Steps and Parameters\n",
    "To create a SageMaker pipeline, you will first create a `ProcessingStep` using the Data Wrangler processor defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"DataWranglerProcessingStep\",\n",
    "    processor=processor,\n",
    "    inputs=processing_job_inputs, \n",
    "    outputs=processing_job_outputs,\n",
    "    job_arguments=[f\"--output-config '{json.dumps(processing_job_output_config)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now add a `TrainingStep` to the pipeline that trains a model on the preprocessed train data set. \n",
    "\n",
    "You can also add more steps. To learn more about adding steps to a pipeline, see [Define a Pipeline](http://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html) in the SageMaker documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "xgb_input_content_type = None\n",
    "\n",
    "if processing_job_output_content_type == \"CSV\":\n",
    "    xgb_input_content_type = 'text/csv'\n",
    "elif processing_job_output_content_type == \"Parquet\":\n",
    "    xgb_input_content_type = 'application/x-parquet'\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"DataWranglerTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=data_wrangler_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                output_name\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=xgb_input_content_type\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "        name=f\"DataWranglerRegisterModel\",\n",
    "        estimator=xgb_train,\n",
    "        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=\"DataWrangler-PackageGroup\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "Now you will create the SageMaker pipeline that combines the steps created above so it can be executed. \n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The parameters supported in this notebook includes:\n",
    "\n",
    "- `instance_type` - The ml.* instance type of the processing job.\n",
    "- `instance_count` - The instance count of the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "# Define Pipeline Parameters\n",
    "instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a pipeline with the steps and parameters defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a unique pipeline name with flow export name\n",
    "pipeline_name = f\"pipeline-{flow_export_name}\"\n",
    "\n",
    "# Combine pipeline steps\n",
    "pipeline_steps = [data_wrangler_step, training_step, register_step]\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[instance_type, instance_count],\n",
    "    steps=pipeline_steps,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Examining the pipeline definition\n",
    "\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and \n",
    "the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the SageMaker Pipeline service and start an execution. The role passed in \n",
    "will be used by the Pipeline service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_role = sagemaker.get_execution_role()\n",
    "pipeline.upsert(role_arn=iam_role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Operations: Examine and Wait for Pipeline Execution\n",
    "\n",
    "Describe the pipeline execution and wait for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the steps in the execution. These are the steps in the pipeline that have been resolved by the step \n",
    "executor service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the pipeline execution status and details in Studio. For details please refer to \n",
    "[View, Track, and Execute SageMaker Pipelines in SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-studio.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Cleanup\n",
    "\n",
    "## Pipeline cleanup\n",
    "Set `pipeline_deletion` flag below to `True` to delete the SageMaker Pipelines created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_deletion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_deletion:\n",
    "    pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model cleanup\n",
    "Set `model_deletion` flag below to `True` to delete the SageMaker Model created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deletion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_deletion:\n",
    "    model_package_group_name = register_step.steps[0].model_package_group_name\n",
    "\n",
    "    model_package_list = sm_client.list_model_packages(\n",
    "        ModelPackageGroupName = model_package_group_name\n",
    "    )\n",
    "\n",
    "    for version in range(0,len(model_package_list[\"ModelPackageSummaryList\"])):\n",
    "        sm_client.delete_model_package(\n",
    "            ModelPackageName = model_package_list[\"ModelPackageSummaryList\"][version][\"ModelPackageArn\"]\n",
    "        )\n",
    "\n",
    "    sm_client.delete_model_package_group(\n",
    "        ModelPackageGroupName = model_package_group_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment cleanup\n",
    "Set `experiment_deletion` flag below to `True` to delete the SageMaker Experiment and Trials created by the Pipeline execution in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_deletion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_deletion:\n",
    "    experiment_name = pipeline_name\n",
    "    trial_name = execution.arn.split(\"/\")[-1]\n",
    "    \n",
    "    components_in_trial = sm_client.list_trial_components(TrialName=trial_name)\n",
    "    print('TrialComponentNames:')\n",
    "    for component in components_in_trial['TrialComponentSummaries']:\n",
    "        component_name = component['TrialComponentName']\n",
    "        print(f\"\\t{component_name}\")\n",
    "        sm_client.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "        try:\n",
    "            # comment out to keep trial components\n",
    "            sm_client.delete_trial_component(TrialComponentName=component_name)\n",
    "        except:\n",
    "            # component is associated with another trial\n",
    "            continue\n",
    "        # to prevent throttling\n",
    "        time.sleep(.5)\n",
    "    sm_client.delete_trial(TrialName=trial_name)\n",
    "    try:\n",
    "        sm_client.delete_experiment(ExperimentName=experiment_name)\n",
    "        print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "    except:\n",
    "        # experiment already existed and had other trials\n",
    "        print(f\"\\nExperiment {experiment_name} in use by other trials. Will not delete\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
